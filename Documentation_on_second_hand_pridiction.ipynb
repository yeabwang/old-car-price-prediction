{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk+wJGP71fd0kcZhCzGlXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeabwang/old-car-price-prediction/blob/main/Documentation_on_second_hand_pridiction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation for Second Hand cars price\n",
        "\n",
        "[Go to model](https://colab.research.google.com/drive/1Ic9Fr876bbe1_MP8kVFv8ydA5Q6YWypJ?usp=sharing)\n",
        "\n",
        "\n",
        "Life cycle\n",
        "1. Understanding the task\n",
        "2. Preparing the data and defining error function for the process\n",
        "3. Developing our model\n",
        "4. Error measurement based on the set scale\n",
        "5. Training and optimization\n",
        "6. Performance Measurement\n",
        "7. Validation and Testing\n",
        "8. Taking a corrective measurement\n",
        "\n",
        "\n",
        "\n",
        "# Task description\n",
        "\n",
        "The task is to build a model which will accept various parameters like hourse power, date of manufacture, model, millage and so on and we will predict the price of that car.\n",
        "\n",
        "# Data Preparation\n",
        "\n",
        "During training\n",
        "\n",
        "- We provide an input to output coorelation for the model to see and understand patterns.\n",
        "\n",
        "After training\n",
        "- We provide the input and the model will provide us the pridicted output.\n",
        "\n",
        "# Data Randomization\n",
        "\n",
        "Before we do normalization, we have to randomize our data so that it won't learn to recognize patterns instead of actually learning the merit. Which will cause one of the problems later, overfitting.\n",
        "\n",
        "# Data Normalization\n",
        "\n",
        "Then we can normalize our data through manual normalization by computing the mean and variance across our columns through axis 1 or by letting it adapt(means will get the mean and the stdandard diviation automatically from the provided dataset) and use it to create our normalizer and use that normalizer to normalize our data.\n",
        "\n",
        "Note that here we only have to normalize our input or x value.\n",
        "\n",
        "# Model preparation\n",
        "\n",
        "Our model will be simple linear regression model that works as y=mx+b.\n",
        "\n",
        "There are basically three types of models we can work on when it comes to tensorflow.\n",
        "\n",
        "1. Sequential API - for simple architectures.\n",
        "2. Functional API - for complex workflows.\n",
        "3. Model subclassing - for full customization.\n",
        "\n",
        "Sequential API: Its the simplest and the most straightforward way of creating deep learning models.\n",
        "- It stack the layers in linear order.\n",
        "- The layers are added sequentially.\n",
        "- One output will be directly passed to the other.\n",
        "\n",
        "There are several layers we can work with in sequential api\n",
        "\n",
        "1. Dense Layer:\n",
        "\n",
        "tf.keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros')\n",
        "\n",
        "- units: Number of neurons in the layer (e.g., 128).\n",
        "Determines the dimensionality of the output space.\n",
        "\n",
        "- activation: Specifies the activation function to be applied to the output of this layer.\n",
        "Examples:\n",
        "'relu': Rectified Linear Unit, used to add non-linearity.\n",
        "'softmax': For multi-class classification.\n",
        "'sigmoid': For binary classification or output probabilities.\n",
        "None: No activation is applied.\n",
        "\n",
        "- use_bias: Defaults to True.\n",
        "If set to False, the layer will not add a bias term.\n",
        "\n",
        "- kernel_initializer: Initializes the weights of the layer.\n",
        "Default is 'glorot_uniform' (Xavier initialization).\n",
        "Other options include 'random_normal' or 'he_uniform'.\n",
        "\n",
        "- bias_initializer: Initializes the bias values.\n",
        "Default is 'zeros'.\n",
        "\n",
        "2. Drop out layer\n",
        "\n",
        "tf.keras.layers.Dropout(rate, noise_shape=None, seed=None)\n",
        "\n",
        "- rate: Fraction of input units to drop.\n",
        "Example: 0.2 means 20% of neurons will be randomly set to zero during training.\n",
        "\n",
        "- noise_shape: An optional parameter to specify which dimensions to apply the dropout. Rarely used in simple architectures.\n",
        "\n",
        "seed: Sets the random seed for reproducibility during training.\n",
        "\n",
        "3. Conv2D Layer (Convolution Layer)\n",
        "\n",
        "tf.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None, input_shape=None)\n",
        "\n",
        "- filters: The number of filters (or kernels) the layer learns.\n",
        "Each filter detects a specific feature (e.g., edges, textures).\n",
        "- kernel_size: Size of the convolutional filter (e.g., (3, 3)).\n",
        "Smaller kernels are common (like 3x3 or 5x5).\n",
        "- strides: Specifies the movement of the filter across the input image. Default is (1, 1) (filter moves by one pixel horizontally and vertically).\n",
        "- strides: Specifies the movement of the filter across the input image. Default is (1, 1) (filter moves by one pixel horizontally and vertically).\n",
        "- activation - the activation function we are going to use.\n",
        "- input_shape: Shape of the input data (required only for the first layer). For an image with dimensions 64x64 and 3 channels (RGB), this would be (64, 64, 3). For the grey scale its (32,32,3).\n",
        "\n",
        "4. MaxPooling2D Layer -\n",
        "\n",
        "tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid')\n",
        "\n",
        "- pool_size:Size of the pooling window (e.g., (2, 2) means a 2x2 window).\n",
        "- strides: Movement of the pooling window. Defaults to pool_size.\n",
        "- padding:'valid': No padding (output size shrinks). 'same': Padding to preserve output dimensions.\n",
        "\n",
        "5.  Flatten Layer -\n",
        "tf.keras.layers.Flatten(data_format=None)\n",
        "\n",
        "- data_format:Specifies how input dimensions are ordered (e.g., 'channels_last' or 'channels_first').\n",
        "Default is 'channels_last'.\n",
        "\n",
        "Combined example\n",
        "```\n",
        "# This is formatted as code\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',               # Optimizer\n",
        "              loss='sparse_categorical_crossentropy',  # Loss function for classification\n",
        "              metrics=['accuracy'])           # Metrics to track\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "Functional API\n",
        "For the complex models, we will see this later.\n",
        "\n",
        "Model Subclassing\n",
        "Gives full control over our training.\n",
        "\n",
        "# Error Measurement\n",
        "In regression tasks, we mainly use mean square error function as our loss function, which wil substruct the actual output from the predicted output and square the result. This will aplify the error for the model to correct it.\n",
        "\n",
        "The other type of loss function for regression tasks is, mean absolute error, where we take the absolute value of the difference between the predicted and the actual output.\n",
        "\n",
        "The correction will be done through a process called back propagation where , which is the old parameters(the weight and bias) - learning rate times the gradiant of change in loss function.\n",
        "\n",
        "Mean absolute error - we use this in cases where we have small outliers which have a high differnece causing a wrong calibiration of our loss function over penalizing as the large error will be aplified.      \n",
        "\n",
        "\n",
        "So to deal with this types of situations effectively we can use the Huber loss function.\n",
        "\n",
        "Huber loss is a piece wise function which uses the MeanSquaredError function when the differece between the actual and predicted value is under certain value.\n",
        "\n",
        "And it uses the MeanAbsoluteError function when the difference between the two passes certain threshold.\n",
        "\n",
        "# Training and optimization\n",
        "There are two main ways of optimizing our models.\n",
        "\n",
        "!. Stochastic Gradient Descent(SGD)\n",
        "\n",
        " We use this optimizer when we are dealing with simple models and we care about simplicity and easy generalizability.\n",
        "\n",
        "θnew = θold - η⋅∇J(θ)\n",
        "\n",
        "where η is the learning rate which goes from 10 ^-1 to 10 ^-6. We have to be careful with this as it affects our training hugely.\n",
        "∇J(θ) is a derivative of the loss function in respect to the weight.\n",
        "\n",
        "Has slow convergence.\n",
        "\n",
        "2. Adam Optimize:  Combines the benefits of momentum and adaptive learning rates: Momentum: Remembers past gradients to smooth updates. Adaptive Learning Rate: Scales the learning rate for each parameter based on its variance.\n",
        "\n",
        "\n",
        "θnew = θold - η⋅m/sqrt(v)+e\n",
        "\n",
        "- m is the means of gradiants\n",
        "- v is the variance of gradiants\n",
        "- e is small constant to avoid division by zero.\n",
        "\n",
        "Has fast convergence.\n",
        "We use it when we are dealing with large and complex models.\n",
        "\n",
        "Training continues till our model converges.\n",
        "\n",
        "Convergence - refers to the point at which a model's training process reaches a state where further iterations produce little or no improvement in its performance. This means our loss function stablizes around the min.\n"
      ],
      "metadata": {
        "id": "fClWCIUiOd0L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cm0kjKQdOwLR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}